{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.06 seconds\n",
      "AI Response: \n",
      "\n",
      "=== Interaction History ===\n",
      "\n",
      "History started at: 2025-02-18T13:19:49.209035\n",
      "Total interactions: 9\n",
      "\n",
      "#1 [2025-02-18T13:19:48.257156]\n",
      "PROMPT: Explain quantum computing basics\n",
      "RESPONSE: Ollama Error [1]: Error: unknown flag: --num-predict\n",
      "!! ERROR: Ollama Error [1]: Error: unknown flag: --num-predict\n",
      "TIME: 0s\n",
      "\n",
      "#2 [2025-02-18T13:19:49.210949]\n",
      "PROMPT: What's the capital of France?\n",
      "RESPONSE: Ollama Error [1]: Error: unknown flag: --num-predict\n",
      "!! ERROR: Ollama Error [1]: Error: unknown flag: --num-predict\n",
      "TIME: 0s\n",
      "\n",
      "#3 [2025-02-18T13:19:49.248124]\n",
      "PROMPT: How does photosynthesis work?\n",
      "RESPONSE: Ollama Error [1]: Error: unknown flag: --num-predict\n",
      "!! ERROR: Ollama Error [1]: Error: unknown flag: --num-predict\n",
      "TIME: 0s\n",
      "\n",
      "#4 [2025-02-18T13:19:49.278495]\n",
      "PROMPT: This is a very long prompt to test how ollama handles long inputs.  It should include many words and characters to ensure the system is robust.  Let's see what happens!\n",
      "RESPONSE: Ollama Error [1]: Error: unknown flag: --num-predict\n",
      "!! ERROR: Ollama Error [1]: Error: unknown flag: --num-predict\n",
      "TIME: 0s\n",
      "\n",
      "#5 [2025-02-18T13:29:40.394545]\n",
      "PROMPT: Explain quantum computing basics\n",
      "RESPONSE: Ollama Error [1]: Error: unknown flag: --num-predict\n",
      "!! ERROR: Ollama Error [1]: Error: unknown flag: --num-predict\n",
      "TIME: 0s\n",
      "\n",
      "#6 [2025-02-18T13:29:40.452093]\n",
      "PROMPT: What's the capital of France?\n",
      "RESPONSE: Ollama Error [2]: /bin/sh: -c: line 0: unexpected EOF while looking for matching `''\n",
      "/bin/sh: -c: line 1: syntax error: unexpected end of file\n",
      "!! ERROR: Ollama Error [2]: /bin/sh: -c: line 0: unexpected EOF while looking for matching `''\n",
      "/bin/sh: -c: line 1: syntax error: unexpected end of file\n",
      "TIME: 0s\n",
      "\n",
      "#7 [2025-02-18T13:29:40.464653]\n",
      "PROMPT: How does photosynthesis work?\n",
      "RESPONSE: Ollama Error [1]: Error: unknown flag: --num-predict\n",
      "!! ERROR: Ollama Error [1]: Error: unknown flag: --num-predict\n",
      "TIME: 0s\n",
      "\n",
      "#8 [2025-02-18T15:38:45.980168]\n",
      "PROMPT: Explain the benefits of running models locally.\n",
      "RESPONSE: \n",
      "TIME: 0.1s\n",
      "\n",
      "#9 [2025-02-18T15:52:45.385878]\n",
      "PROMPT: Explain the benefits of running models locally.\n",
      "RESPONSE: \n",
      "TIME: 0.06s\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Constant for model definition\n",
    "MODEL_NAME = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\"\n",
    "HISTORY_FILE = \"ollama_history.pkl\"\n",
    "\n",
    "def save_to_pickle(response_data):\n",
    "    \"\"\"Save response data to pickle file with timestamp\"\"\"\n",
    "    try:\n",
    "        with open(HISTORY_FILE, \"rb\") as f:\n",
    "            history = pickle.load(f)\n",
    "    except (FileNotFoundError, EOFError):\n",
    "        history = {\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'interactions': []\n",
    "        }\n",
    "    \n",
    "    history['interactions'].append(response_data)\n",
    "    \n",
    "    with open(HISTORY_FILE, \"wb\") as f:\n",
    "        pickle.dump(history, f)\n",
    "\n",
    "def run_local_ollama_model(prompt, max_tokens=100, timeout=60):\n",
    "    \"\"\"\n",
    "    Runs the specified model with controlled response length.\n",
    "    Now uses the predefined MODEL_NAME constant\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    response_data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model': MODEL_NAME,\n",
    "        'prompt': prompt,\n",
    "        'response': '',\n",
    "        'execution_time': 0,\n",
    "        'error': None\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        constrained_prompt = f\"{prompt} Please answer concisely and keep responses under {max_tokens} tokens.\"\n",
    "        \n",
    "        process = subprocess.run(\n",
    "            [\"ollama\", \"run\", MODEL_NAME, \"--num-predict\", str(max_tokens)],\n",
    "            input=constrained_prompt,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout\n",
    "        )\n",
    "\n",
    "        response = process.stdout.strip()\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        response_data.update({\n",
    "            'response': response,\n",
    "            'execution_time': round(elapsed_time, 2)\n",
    "        })\n",
    "        \n",
    "        save_to_pickle(response_data)\n",
    "        print(f\"Execution time: {elapsed_time:.2f} seconds\")\n",
    "        return response\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        response_data['response'] = \"Error: Response timed out.\"\n",
    "        save_to_pickle(response_data)\n",
    "        return response_data['response']\n",
    "\n",
    "# Example usage remains identical\n",
    "if __name__ == \"__main__\":\n",
    "    prompt_text = \"Explain the benefits of running models locally.\"\n",
    "    \n",
    "    # Get response with max 50 tokens\n",
    "    response = run_local_ollama_model(prompt_text, max_tokens=50)\n",
    "    print(\"AI Response:\", response)\n",
    "    \n",
    "    # View history\n",
    "    print(\"\\n=== Interaction History ===\")\n",
    "    load_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Explain quantum computing in simple terms\n",
      "Executing command: ollama run hf.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF:Q3_K_S ''Explain quantum computing in simple terms'' --max-tokens 200\n",
      "Ollama Error Output:\n",
      "Error: unknown flag: --max-tokens\n",
      "\n",
      "Response: \n",
      "\n",
      "Query: Write a Python function to calculate Fibonacci sequence\n",
      "Executing command: ollama run hf.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF:Q3_K_S ''Write a Python function to calculate Fibonacci sequence'' --max-tokens 200\n",
      "Ollama Error Output:\n",
      "Error: unknown flag: --max-tokens\n",
      "\n",
      "Response: \n",
      "\n",
      "Query: What are the main benefits of renewable energy?\n",
      "Executing command: ollama run hf.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF:Q3_K_S ''What are the main benefits of renewable energy?'' --max-tokens 200\n",
      "Ollama Error Output:\n",
      "Error: unknown flag: --max-tokens\n",
      "\n",
      "Response: \n",
      "\n",
      "=== hf.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF:Q3_K_S History (Local Ollama Model) ===\n",
      "Created: 2025-02-18T15:48:42.015846\n",
      "Total Interactions: 9\n",
      "\n",
      "Interaction #1 (2025-02-18T15:48:41.895006)\n",
      "Prompt: Explain quantum computing in simple terms\n",
      "Response: \n",
      "Error: Model Error [1]: Error: unknown flag: --max-tokens\n",
      "Stats: 0s | 0 tokens\n",
      "\n",
      "Interaction #2 (2025-02-18T15:48:42.017800)\n",
      "Prompt: Write a Python function to calculate Fibonacci sequence\n",
      "Response: \n",
      "Error: Model Error [1]: Error: unknown flag: --max-tokens\n",
      "Stats: 0s | 0 tokens\n",
      "\n",
      "Interaction #3 (2025-02-18T15:48:42.322228)\n",
      "Prompt: What are the main benefits of renewable energy?\n",
      "Response: \n",
      "Error: Model Error [1]: Error: unknown flag: --max-tokens\n",
      "Stats: 0s | 0 tokens\n",
      "\n",
      "Interaction #4 (2025-02-18T15:51:53.320193)\n",
      "Prompt: Explain quantum computing in simple terms\n",
      "Response: \n",
      "Error: Model Error [1]: Error: unknown flag: --max-tokens\n",
      "Stats: 0s | 0 tokens\n",
      "\n",
      "Interaction #5 (2025-02-18T15:51:53.361628)\n",
      "Prompt: Write a Python function to calculate Fibonacci sequence\n",
      "Response: \n",
      "Error: Model Error [1]: Error: unknown flag: --max-tokens\n",
      "Stats: 0s | 0 tokens\n",
      "\n",
      "Interaction #6 (2025-02-18T15:51:53.404849)\n",
      "Prompt: What are the main benefits of renewable energy?\n",
      "Response: \n",
      "Error: Model Error [1]: Error: unknown flag: --max-tokens\n",
      "Stats: 0s | 0 tokens\n",
      "\n",
      "Interaction #7 (2025-02-18T15:51:59.364390)\n",
      "Prompt: Explain quantum computing in simple terms\n",
      "Response: \n",
      "Error: Model Error [1]: Error: unknown flag: --max-tokens\n",
      "Stats: 0s | 0 tokens\n",
      "\n",
      "Interaction #8 (2025-02-18T15:51:59.432330)\n",
      "Prompt: Write a Python function to calculate Fibonacci sequence\n",
      "Response: \n",
      "Error: Model Error [1]: Error: unknown flag: --max-tokens\n",
      "Stats: 0s | 0 tokens\n",
      "\n",
      "Interaction #9 (2025-02-18T15:51:59.479778)\n",
      "Prompt: What are the main benefits of renewable energy?\n",
      "Response: \n",
      "Error: Model Error [1]: Error: unknown flag: --max-tokens\n",
      "Stats: 0s | 0 tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import shlex\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"hf.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF:Q3_K_S\"\n",
    "HISTORY_FILE = \"phi3_interactions.pkl\"\n",
    "\n",
    "def save_interaction(response_data):\n",
    "    \"\"\"Saves interaction data to a pickle file.\"\"\"\n",
    "    try:\n",
    "        with open(HISTORY_FILE, \"rb\") as f:\n",
    "            history = pickle.load(f)\n",
    "    except (FileNotFoundError, EOFError):\n",
    "        history = {\n",
    "            'model': MODEL_NAME,\n",
    "            'created': datetime.now().isoformat(),\n",
    "            'interactions': []\n",
    "        }\n",
    "\n",
    "    history['interactions'].append(response_data)\n",
    "    \n",
    "    with open(HISTORY_FILE, \"wb\") as f:\n",
    "        pickle.dump(history, f)\n",
    "\n",
    "def run_phi3_query(prompt, max_tokens=150, timeout=120):\n",
    "    \"\"\"Runs a query against the *local* Phi-3 model using Ollama.\"\"\"\n",
    "    start_time = time.time()\n",
    "    interaction = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'prompt': prompt,\n",
    "        'response': '',\n",
    "        'tokens_used': 0,\n",
    "        'execution_time': 0,\n",
    "        'error': None\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        cmd = f\"ollama run {MODEL_NAME} '{shlex.quote(prompt)}' --max-tokens {max_tokens}\"\n",
    "        \n",
    "        print(f\"Executing command: {cmd}\")\n",
    "\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout\n",
    "        )\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            response = result.stdout.strip()\n",
    "            interaction.update({\n",
    "                'response': response,\n",
    "                'tokens_used': len(response.split()),\n",
    "                'execution_time': round(time.time() - start_time, 2)\n",
    "            })\n",
    "        else:\n",
    "            error_msg = f\"Model Error [{result.returncode}]: {result.stderr.strip()}\"\n",
    "            interaction['error'] = error_msg\n",
    "            print(f\"Ollama Error Output:\\n{result.stderr}\")\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        error_msg = f\"Timeout after {timeout}s\"\n",
    "        interaction['error'] = error_msg\n",
    "    except Exception as e:\n",
    "        error_msg = f\"System Error: {str(e)}\"\n",
    "        interaction['error'] = error_msg\n",
    "\n",
    "    save_interaction(interaction)\n",
    "    return interaction\n",
    "\n",
    "def show_history():\n",
    "    \"\"\"Displays the stored interaction history.\"\"\"\n",
    "    try:\n",
    "        with open(HISTORY_FILE, \"rb\") as f:\n",
    "            history = pickle.load(f)\n",
    "            print(f\"\\n=== {MODEL_NAME} History (Local Ollama Model) ===\")\n",
    "            print(f\"Created: {history['created']}\")\n",
    "            print(f\"Total Interactions: {len(history['interactions'])}\\n\") # Corrected f-string here\n",
    "            \n",
    "            for idx, entry in enumerate(history['interactions'], 1):\n",
    "                print(f\"Interaction #{idx} ({entry['timestamp']})\")\n",
    "                print(f\"Prompt: {entry['prompt']}\")\n",
    "                print(f\"Response: {entry['response'][:200]}{'...' if len(entry['response']) > 200 else ''}\")\n",
    "                if entry['error']:\n",
    "                    print(f\"Error: {entry['error']}\")\n",
    "                print(f\"Stats: {entry['execution_time']}s | {entry['tokens_used']} tokens\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No interaction history found\")\n",
    "\n",
    "\n",
    "# Example Usage in Jupyter Notebook:\n",
    "prompts = [\n",
    "    \"Explain quantum computing in simple terms\",\n",
    "    \"Write a Python function to calculate Fibonacci sequence\",\n",
    "    \"What are the main benefits of renewable energy?\",\n",
    "]\n",
    "\n",
    "for query in prompts:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    result = run_phi3_query(query, max_tokens=200)\n",
    "    print(f\"Response: {result['response'][:100]}{'...' if len(result['response']) > 100 else ''}\")\n",
    "\n",
    "show_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.19 seconds\n",
      "AI Response: \n"
     ]
    }
   ],
   "source": [
    "# works creates pickle\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_local_ollama_model(model_name, prompt, max_tokens=100, timeout=60):\n",
    "    \"\"\"\n",
    "    Runs a local Ollama model with controlled response length.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Model name (e.g., \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\")\n",
    "        prompt (str): Input prompt for the model\n",
    "        max_tokens (int): Maximum number of tokens in response (default: 100)\n",
    "        timeout (int): Maximum time to wait (seconds)\n",
    "\n",
    "    Returns:\n",
    "        str: Model's output or error message\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Add concise instruction to the prompt\n",
    "        constrained_prompt = f\"{prompt} Please answer concisely and keep responses under {max_tokens} tokens.\"\n",
    "        \n",
    "        process = subprocess.run(\n",
    "            [\"ollama\", \"run\", model_name, \"--num-predict\", str(max_tokens)],\n",
    "            input=constrained_prompt,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout\n",
    "        )\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Error: Response timed out.\"\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Execution time: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return process.stdout.strip()\n",
    "\n",
    "# Example usage with length constraints\n",
    "model = \"hf.co/bartowski/Phi-3.5-mini-instruct_Uncensored-GGUF:Q3_K_S\"\n",
    "prompt_text = \"Explain the benefits of running models locally.\"\n",
    "\n",
    "# Get response with max 50 tokens\n",
    "response = run_local_ollama_model(model, prompt_text, max_tokens=50)\n",
    "print(\"AI Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìú Conversation History\n",
      "Error reading history: 'created_at'\n"
     ]
    }
   ],
   "source": [
    "def display_history():\n",
    "    \"\"\"Display the complete interaction history from pickle file\"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    try:\n",
    "        with open(HISTORY_FILE, \"rb\") as f:\n",
    "            history = pickle.load(f)\n",
    "            \n",
    "            print(f\"üìú Conversation History\")\n",
    "            print(f\"üîñ Created at: {datetime.fromisoformat(history['created_at']).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"Total interactions: {len(history['interactions'])}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for idx, interaction in enumerate(history['interactions'], 1):\n",
    "                print(f\"\\nüîÑ Interaction #{idx}\")\n",
    "                print(f\"‚è∞ Time: {datetime.fromisoformat(interaction['timestamp']).strftime('%m/%d %H:%M:%S')}\")\n",
    "                print(f\"ü§ñ Model: {interaction['model']}\")\n",
    "                print(f\"üìù Prompt: {interaction['prompt']}\")\n",
    "                print(f\"üí° Response: {interaction['response']}\")\n",
    "                if interaction['error']:\n",
    "                    print(f\"‚ùå Error: {interaction['error']}\")\n",
    "                print(f\"‚è±Ô∏è Duration: {interaction['execution_time']}s\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        print(\"No history found - create your first interaction!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading history: {str(e)}\")\n",
    "\n",
    "# Run this in a new cell after saving some interactions\n",
    "display_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
